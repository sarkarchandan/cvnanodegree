{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Part-of-Speech Tagging\n",
    "\n",
    "In this section, we will use an LSTM to predict part-of-speech tags for words. What exactly is part-of-speech tagging?\n",
    "\n",
    "Part of speech tagging is the process of determining the *category* of a word from the words in its surrounding context. You can think of part of speech tagging as a way to go from words to their [Mad Libs](https://en.wikipedia.org/wiki/Mad_Libs#Format) categories. Mad Libs are incomplete short stories that have many words replaced by blanks. Each blank has a specified word-category, such as `\"noun\"`, `\"verb\"`, `\"adjective\"`, and so on. One player asks another to fill in these blanks (prompted only by the word-category) until they have created a complete, silly story of their own. Here is an example of such categories:\n",
    "\n",
    "```text\n",
    "Today, you'll be learning how to [verb]. It may be a [adjective] process, but I think it will be rewarding! \n",
    "If you want to take a break you should [verb] and treat yourself to some [plural noun].\n",
    "```\n",
    "... and a set of possible words that fall into those categories:\n",
    "```text\n",
    "Today, you'll be learning how to code. It may be a challenging process, but I think it will be rewarding! \n",
    "If you want to take a break you should stretch and treat yourself to some puppies.\n",
    "```\n",
    "\n",
    "\n",
    "### Why Tag Speech?\n",
    "\n",
    "Tagging parts of speech is often used to help disambiguate natural language phrases because it can be done quickly and with high accuracy. It can help answer: what subject is someone talking about? Tagging can be used for many NLP tasks like creating new sentences using a sequence of tags that make sense together, filling in a Mad Libs style game, and determining correct pronunciation during speech synthesis. It is also used in information retrieval, and for word disambiguation (ex. determining when someone says *right* like the direction versus *right* like \"that's right!\").\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Now, we know that neural networks do not do well with words as input and so our first step will be to prepare our training data and map each word to a numerical value. \n",
    "\n",
    "We start by creating a small set of training data, you can see that this is a few simple sentences broken down into a list of words and their corresponding word-tags. Note that the sentences are turned into lowercase words using `lower()` and then split into separate words using `split()`, which splits the sentence by whitespace characters.\n",
    "\n",
    "#### Words to indices\n",
    "\n",
    "Then, from this training data, we create a dictionary that maps each unique word in our vocabulary to a numerical value; a unique index `idx`. We do the same for each word-tag, for example: a noun will be represented by the number `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device: str = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sentences and their corresponding word-tags\n",
    "DataType = List[Tuple[List[str], List[str]]]\n",
    "\n",
    "training_data: DataType = [\n",
    "    (\"The cat ate the cheese\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"She read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"The dog loves art\".lower().split(), [\"DET\", \"NN\", \"V\", \"NN\"]),\n",
    "    (\"The elephant answers the phone\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# create a dictionary that maps words to indices\n",
    "word2idx: Dict[str, int]  = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        # Checks and only inserts the word in \n",
    "        # dictionary if it is not prepsent already.\n",
    "        # Dictionary must have a single occurrence of \n",
    "        # a given word from the training data set.\n",
    "        if word not in word2idx:\n",
    "            # Ensures, that a word to be inserted in \n",
    "            # the dictionary gets an index according to \n",
    "            # the point in time, in which it is being \n",
    "            # inserted. For the very starting word length \n",
    "            # of the dictionary is 0, for the second word \n",
    "            # length is 1 because the dictionary already \n",
    "            # has the first word and so on.\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# create a dictionary that maps tags to indices\n",
    "tag2idx: Dict[str, int] = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print out the created dictionary to see the words and their numerical values! \n",
    "\n",
    "You should see every word in our training set and its index value. Note that the word \"the\" only appears once because our vocabulary only includes *unique* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index dictionary, must have a single occurrence for each word from the training dataset:\n",
      "{'the': 0, 'cat': 1, 'ate': 2, 'cheese': 3, 'she': 4, 'read': 5, 'that': 6, 'book': 7, 'dog': 8, 'loves': 9, 'art': 10, 'elephant': 11, 'answers': 12, 'phone': 13}\n"
     ]
    }
   ],
   "source": [
    "# Print out the created dictionary\n",
    "print(f\"Word to index dictionary, must have a single occurrence for each word from the training dataset:\\n{word2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for converting a sequence of words to a Tensor of numerical values\n",
    "# will be used later in training\n",
    "def prepare_sequence(seq: List[str], to_idx: Dict[str, int]) -> torch.Tensor:\n",
    "    '''This function takes in a sequence of words and returns a \n",
    "    corresponding Tensor of numerical values (indices for each word).'''\n",
    "    idxs: List[int] = [to_idx[w] for w in seq]\n",
    "    idxs: np.ndarray = np.array(idxs)\n",
    "    return torch.from_numpy(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  8, 12,  0, 13], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# check out what prepare_sequence does for one of our training sentences:\n",
    "example_input: torch.Tensor = prepare_sequence(seq=\"The dog answers the phone\".lower().split(), to_idx=word2idx)\n",
    "print(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "---\n",
    "\n",
    "Our model will assume a few things:\n",
    "1. Our input is broken down into a sequence of words, so a sentence will be [w1, w2, ...]\n",
    "2. These words come from a larger list of words that we already know (a vocabulary)\n",
    "3. We have a limited set of tags, `[NN, V, DET]`, which mean: a noun, a verb, and a determinant (words like \"the\" or \"that\"), respectively\n",
    "4. We want to predict\\* a tag for each input word\n",
    "\n",
    "\\* To do the prediction, we will pass an LSTM over a test sentence and apply a softmax function to the hidden state of the LSTM; the result is a vector of tag scores from which we can get the predicted tag for a word based on the *maximum* value in this distribution of tag scores. \n",
    "\n",
    "Mathematically, we can represent any tag prediction $\\hat{y}_i$ as: \n",
    "\n",
    "$\\hat{y}_i$ = $argmax_j (log Softmax(Ah_i + b))_j$\n",
    "\n",
    "Where $A$ is a learned weight and $b$, a learned bias term, and the hidden state at timestep $i$ is $h_i$. \n",
    "\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "We know that an LSTM takes in an expected input size and hidden size, but sentences are rarely of a consistent size, so how can we define the input of our LSTM?\n",
    "\n",
    "Well, at the very start of this net, we'll create an `Embedding` layer that takes in the size of our vocabulary and returns a vector of a specified size, `embedding_dim`, for each word in an input sequence of words. It's important that this be the first layer in this net. You can read more about this embedding layer in [the PyTorch documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#word-embeddings-in-pytorch).\n",
    "\n",
    "Pictured below is the expected architecture for this tagger model.\n",
    "\n",
    "<img src='images/speech_tagger.png' height=60% width=60% >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM nwtwork model\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    # First embedding layer to ensure, that the input word \n",
    "    # tensor at each time step has the consistent size.\n",
    "    word_embeddings: nn.Embedding\n",
    "    # Defines the LSTM layer encapsulating the hidden state.\n",
    "    lstm: nn.LSTM\n",
    "    # Defines a linear layer implementing the log softamx \n",
    "    # to convert short term memory or out to class scores.\n",
    "    hidden2tag: nn.Linear\n",
    "    # Consistent length of word tensors at each input step.\n",
    "    hidden_size: int\n",
    "    # Hidden state for the LSTM layer. Keeps track of short \n",
    "    # and long term memories.\n",
    "    hidden: Tuple[torch.Tensor, torch.Tensor]\n",
    "    batch_size: int\n",
    "\n",
    "    def __init__(self, embedding_dim: int, hidden_size: int, vocab_size: int, target_size: int, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initializes model state.\n",
    "        :param embedding_dim: Intended size of embedding vector. Same as the input size for \n",
    "            LSTM layer at each time step.\n",
    "        :type embedding_dim: int\n",
    "        :param hidden_size: Size of the hidden state for LSTM. Analogous to the number of \n",
    "            hidden neurons at each LSTM time step.\n",
    "        :type hidden_size: int\n",
    "        :param vocab_size: Size of the dictionary for embedding. Needed for initializing \n",
    "            the embedding layer.\n",
    "        :type vocab_size: int\n",
    "        :param target_size: Number of expected class scores. Same as the out features size \n",
    "            of the linear layer converting short term memory from LSTM to class scores at \n",
    "            each time step.\n",
    "        :type target_size: int\n",
    "        :param batch_size: Batch size for sentences as training examples.\n",
    "        :type batch_size: int\n",
    "        \"\"\"\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs \n",
    "        # and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "\n",
    "        # the linear layer that maps the hidden state output dimension \n",
    "        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(in_features=hidden_size, out_features=target_size)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"At the start of training, we need to initialize a hidden state;\n",
    "        there will be none because the hidden state is formed based on perviously seen data.\n",
    "        So, this function defines a hidden state with all zeroes and of a specified size.\n",
    "        \"\"\"\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_size).to(device=Device),\n",
    "                torch.zeros(1, 1, self.hidden_size).to(device=Device))\n",
    "\n",
    "    def forward(self, sentence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Defines the feed forward behavior.\n",
    "        We input the sentence with all the time steps together like \n",
    "        the second example of using the LSTM layer in the previous \n",
    "        notebook.\n",
    "        \"\"\"\n",
    "        # Create embedded word vectors for each word in a sentence\n",
    "        embedded: torch.Tensor = self.word_embeddings(sentence)\n",
    "        \n",
    "        # Get the output and hidden state by passing the LSTM over our word embeddings\n",
    "        # the LSTM takes in our embeddings and hiddent state. The hidden state is \n",
    "        # initialized with zeros. Our batch size is 1.\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(sentence), self.batch_size, -1), self.hidden)\n",
    "        \n",
    "        # Get the scores for the most likely tag for a word. One confusion may arise \n",
    "        # here, that we have initialized the linear layer to have the input features \n",
    "        # size same as the hidden size. When we are reshaping the lstm_out like below \n",
    "        # how it is maintained, that the shape guarantee would be maintained. Actually \n",
    "        # in this case, that happens because we have assumed the hidden size and input \n",
    "        # size to be same and in order to enforce, that we have used word embedding.\n",
    "        # For instance, if embedded has the shape (5, 1, 6) including the batch_size, \n",
    "        # then lstm_out would also have the shape (5, 1, 6), on which we can then squeeze \n",
    "        # the middle dimension and use for computing log softmax.\n",
    "        tag_outputs: torch.Tensor = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores: torch.Tensor = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how the model trains\n",
    "\n",
    "To train the model, we have to instantiate it and define the loss and optimizers that we want to use.\n",
    "\n",
    "First, we define the size of our word embeddings. The `EMBEDDING_DIM` defines the size of our word vectors for our simple vocabulary and training set; we will keep them small so we can see how the weights change as we train.\n",
    "\n",
    "**Note: the embedding dimension for a complex dataset will usually be much larger, around 64, 128, or 256 dimensional.**\n",
    "\n",
    "\n",
    "#### Loss and Optimization\n",
    "\n",
    "Since our LSTM outputs a series of tag scores with a softmax layer, we will use `NLLLoss`. In tandem with a softmax layer, NLL Loss creates the kind of cross entropy loss that we typically use for analyzing a distribution of class scores. We could have used `CrossEntropyLoss` as well but since we have already defined the `log_softamx` ourselves, using `NLLoss` makes sense. We'll use standard gradient descent optimization, but you are encouraged to play around with other optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding dimension defines the size of our word vectors\n",
    "# for our simple vocabulary and training set, we will keep these small\n",
    "EMBEDDING_DIM: int = 6\n",
    "HIDDEN_SIZE: int = 6\n",
    "\n",
    "# Instantiate our model and load to \n",
    "# CUDA.\n",
    "model: LSTMTagger = LSTMTagger(\n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    vocab_size=len(word2idx), \n",
    "    target_size=len(tag2idx)\n",
    ").to(device=Device)\n",
    "\n",
    "# define our loss and optimizer\n",
    "loss_function: nn.NLLLoss = nn.NLLLoss()\n",
    "optimizer: optim.SGD = optim.SGD(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that our model has learned something, let's first look at the scores for a sample test sentence *before* our model is trained. **Note that the test sentence *must* be made of words from our vocabulary otherwise its words cannot be turned into indices**.\n",
    "\n",
    "The scores should be Tensors of length 3 (for each of our tags) and there should be scores for each word in the input sentence.\n",
    "\n",
    "For the test sentence, \"The cheese loves the elephant\", we know that this has the tags (DET, NN, V, DET, NN) or `[0, 1, 2, 0, 1]`, but our network does not yet know this. In fact, in this case, our model starts out with a hidden state of all zeroes and so all the scores and the predicted tags should be low, random, and about what you'd expect for a network that is not yet trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8404, -1.8030, -0.9072],\n",
      "        [-0.8657, -1.7955, -0.8838],\n",
      "        [-0.8825, -1.7909, -0.8688],\n",
      "        [-0.8645, -1.7615, -0.8990],\n",
      "        [-0.8763, -1.7772, -0.8805]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([0, 0, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test_sentence: List[str] = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "# see what the scores are before training\n",
    "# element [i,j] of the output is the *score* for tag j for word i.\n",
    "# to check the initial accuracy of our model, we don't need to train, so we use model.eval()\n",
    "inputs: torch.Tensor = prepare_sequence(test_sentence, word2idx)\n",
    "model.eval()\n",
    "# We need to remember to load the input tensor to CUDA\n",
    "tag_scores: torch.Tensor = model(inputs.to(device=Device))\n",
    "print(tag_scores)\n",
    "\n",
    "# tag_scores outputs a vector of tag scores for each word in an inpit sentence\n",
    "# to get the most likely tag index, we grab the index with the maximum score!\n",
    "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "_, predicted_tags = torch.max(tag_scores.cpu(), 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Model\n",
    "\n",
    "Loop through all our training data for multiple epochs (again we are using a small epoch value for this simple training data). This loop:\n",
    "\n",
    "1. Prepares our model for training by zero-ing the gradients\n",
    "2. Initializes the hidden state of our LSTM\n",
    "3. Prepares our data for training\n",
    "4. Runs a forward pass on our inputs to get tag_scores\n",
    "5. Calculates the loss between tag_scores and the true tag\n",
    "6. Updates the weights of our model using backpropagation\n",
    "\n",
    "In this example, we are printing out the average epoch loss, every 20 epochs; you should see it decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, loss: 0.94861\n",
      "Epoch: 40, loss: 0.72423\n",
      "Epoch: 60, loss: 0.51686\n",
      "Epoch: 80, loss: 0.30964\n",
      "Epoch: 100, loss: 0.15810\n",
      "Epoch: 120, loss: 0.09109\n",
      "Epoch: 140, loss: 0.05951\n",
      "Epoch: 160, loss: 0.04253\n",
      "Epoch: 180, loss: 0.03246\n",
      "Epoch: 200, loss: 0.02598\n",
      "Epoch: 220, loss: 0.02152\n",
      "Epoch: 240, loss: 0.01830\n",
      "Epoch: 260, loss: 0.01588\n",
      "Epoch: 280, loss: 0.01399\n",
      "Epoch: 300, loss: 0.01249\n"
     ]
    }
   ],
   "source": [
    "# normally these epochs take a lot longer \n",
    "# but with our toy data (only 3 sentences), we can do many epochs in a short time\n",
    "n_epochs: int = 300\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss: float = 0.0\n",
    "    \n",
    "    # get all sentences and corresponding tags in the training data\n",
    "    # Since we are processing one sentence at a time our batch size is 1.\n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # zero the hidden state of the LSTM, this detaches it from its history\n",
    "        # IMPORTANT: This is only needed because we did our testing before and \n",
    "        # ad that point the hidden state was initialized and modified once. In \n",
    "        # order to give ourselves a clean slate we need to reinitialize the \n",
    "        # hidden state. Usually this is not required as it already happens during \n",
    "        # the initialization of the model.\n",
    "        model.hidden = model.init_hidden()  # <- This is usually not required\n",
    "\n",
    "        # prepare the inputs from the dataset for processing by out network, \n",
    "        # turn all sentences and targets into Tensors of numerical indices\n",
    "        sentence_in: torch.Tensor = prepare_sequence(sentence, word2idx)\n",
    "        # prepare the ground truth from te dataset in the same way\n",
    "        # IMPORTANT: In order for this to work we have to convert the datatype \n",
    "        # of the ground truth to LongTensor. Because, Embedding layer works \n",
    "        # with that data type. CUDA needs all the datatypes to be consistent.\n",
    "        targets: torch.Tensor = prepare_sequence(tags, tag2idx).type(torch.LongTensor)  # <- This is important for CUDA\n",
    "\n",
    "        # forward pass to get tag scores\n",
    "        tag_scores: torch.Tensor = model(sentence_in.to(device=Device))\n",
    "\n",
    "        # compute the loss, and gradients \n",
    "        loss: torch.Tensor = loss_function(tag_scores, targets.to(device=Device))\n",
    "\n",
    "        epoch_loss += loss.cpu().item()\n",
    "        \n",
    "        # Propagate backward to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the model parameters with optimizer.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print out avg loss per 20 epochs\n",
    "    if(epoch%20 == 19):\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(training_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "See how your model performs *after* training. Compare this output with the scores from before training, above.\n",
    "\n",
    "Again, for the test sentence, \"The cheese loves the elephant\", we know that this has the tags (DET, NN, V, DET, NN) or `[0, 1, 2, 0, 1]`. Let's see if our model has learned to find these tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0442e-02, -4.6424e+00, -7.1905e+00],\n",
      "        [-6.5866e+00, -4.5614e-03, -5.7533e+00],\n",
      "        [-6.8559e+00, -4.0470e+00, -1.8701e-02],\n",
      "        [-8.2205e-03, -4.9494e+00, -6.8129e+00],\n",
      "        [-6.4957e+00, -3.7958e-03, -6.0842e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([0, 1, 2, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_sentence: List[str] = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "# see what the scores are after training\n",
    "input: torch.Tensor = prepare_sequence(test_sentence, word2idx)\n",
    "model.eval()\n",
    "tag_scores: torch.Tensor = model(input.to(device=Device))\n",
    "print(tag_scores)\n",
    "\n",
    "# print the most likely tag index, by grabbing the index with the maximum score!\n",
    "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!\n",
    "\n",
    "To improve this model, see if you can add sentences to this model and create a more robust speech tagger. Try to initialize the hidden state in a different way or play around with the optimizers and see if you can decrease model loss even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
