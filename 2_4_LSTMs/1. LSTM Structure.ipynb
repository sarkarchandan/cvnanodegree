{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Structure and Hidden State\n",
    "\n",
    "We know that RNNs are used to maintain a kind of memory by linking the output of one node to the input of the next. In the case of an LSTM, for each piece of data in a sequence (say, for a word in a given sentence), there is a corresponding *hidden state* $h_t$. This hidden state is a function of the pieces of data that an LSTM has seen over time; it contains some weights and, represents both the short term and long term memory components for the data that the LSTM has already seen. \n",
    "\n",
    "So, for an LSTM that is looking at words in a sentence, **the hidden state of the LSTM will change based on each new word it sees. And, we can use the hidden state to predict the next word in a sequence** or help identify the type of word in a language model, and lots of other things!\n",
    "\n",
    "\n",
    "## LSTMs in Pytorch\n",
    "\n",
    "To create and train an LSTM, you have to know how to structure the inputs, and hidden state of an LSTM. In PyTorch an LSTM can be defined as: `lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)`.\n",
    "\n",
    "In PyTorch, an LSTM expects all of its inputs to be 3D tensors, with dimensions defined as follows:\n",
    ">* `input_size` = the number of inputs (a dimension of 20 could represent 20 inputs)\n",
    ">* `hidden_size` = the size of the hidden state; this will be the number of outputs that each LSTM cell produces at each time step.\n",
    ">* `num_layers ` = the number of hidden LSTM layers to use; this is typically  a value between 1 and 3; a value of 1 means that each LSTM cell has one hidden state. This has a default value of 1.\n",
    "\n",
    "<img src='./images/lstm_simple_ex.png'>\n",
    "    \n",
    "### Hidden State\n",
    "\n",
    "Once an LSTM has been defined with input and hidden dimensions, we can call it and retrieve the output and hidden state at every time step.\n",
    " `out, hidden = lstm(input.view(1, 1, -1), (h0, c0))`. The `hidden` state comprises of new short term memory (can be same as output, as we'd see shortly) and the new long term memory. When we pass on h0 and c0, we are basically supplying the initial values of these two aspects only. Why the new short term memory can be same as output ? It is because as we'd see in the example below, that a single LSTM layer does not represent a model. It is like a complex hidden layer. And in our examples below we have not said, how to process the activations from it. Hence, `out` and the first part of the `hidden` i.e., new short term memory would be one and same in this example when we look them up at every time step.\n",
    "\n",
    "The inputs to an LSTM are **`(input, (h0, c0))`**.\n",
    ">* `input` = a Tensor containing the values in an input sequence; this has values: (seq_len, batch, input_size)\n",
    ">* `h0` = a Tensor containing the initial hidden state (short term memory) for each element in a batch\n",
    ">* `c0` = a Tensor containing the initial cell memory (long term memory) for each element in the batch\n",
    "\n",
    "`h0` and `c0` will default to 0, if they are not specified. Their dimensions are: (n_layers, batch, hidden_dim).\n",
    "\n",
    "These will become clearer in the example in this notebook. This and the following notebook are modified versions of [this PyTorch LSTM tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch).\n",
    "\n",
    "Let's take a simple example and say we want to process a single sentence through an LSTM. If we want to run the sequence model over one sentence \"Giraffes in a field\", our input should look like this `1x4` row vector of individual words:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\text{Giraffes  } \n",
    "   \\text{in  } \n",
    "   \\text{a  } \n",
    "   \\text{field} \n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "In this case, we know that we have **4 inputs words** and we decide how many outputs to generate at each time step, say we want each LSTM cell to generate **3 hidden state values**. We'll keep the number of layers in our LSTM at the default size of 1.\n",
    "\n",
    "The hidden state and cell memory will have dimensions (n_layers, batch, hidden_dim), and in this case that will be (1, 1, 3) for a 1 layer model with one batch/sequence of words to process (this one sentence) and 3 genereated, hidden state values.\n",
    "\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Next, let's see an example of one LSTM that is designed to look at a sequence of 4 values (numerical values since those are easiest to create and track) and generate 3 values as output. This is what the sentence processing network from above will look like, and you are encouraged to change these input/hidden-state sizes to see the effect on the structure of the LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21fb0103f90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2) # so that random variables will be consistent and repeatable for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple LSTM\n",
    "\n",
    "\n",
    "**A note on hidden and output dimensions**\n",
    "\n",
    "The `hidden_dim` and size of the output will be the same unless you define your own LSTM and change the number of outputs by adding a linear layer at the end of the network, ex. fc = nn.Linear(hidden_dim, output_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We would eventually understand, that the nn.LSTM layer spans over the entirety of the timestamps in an example sequence i.e., intuitively it represents an unfolded model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Note about the weights of the LSTM\n",
    "\n",
    "~LSTM.weight_ih_l[k] – the learnable input-hidden weights of the $k^{th}$ layer (W_ii|W_if|W_ig|W_io), of shape (4 \\* hidden_size, input_size) for k = 0. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size). If proj_size > 0 was specified, the shape will be (4*hidden_size, num_directions * proj_size) for k > 0\n",
    "\n",
    "~LSTM.weight_hh_l[k] – the learnable hidden-hidden weights of the $k^{th}$ layer (W_hi|W_hf|W_hg|W_ho), of shape (4 \\* hidden_size, hidden_size). If proj_size > 0 was specified, the shape will be (4*hidden_size, proj_size).\n",
    "\n",
    "The term $k$ can be greater than 0, when we stack LSTMs. In this notebook we are not doing that. Moreover, we notice, that the weights matrices are stacked vertically to compute their compact dimensions. In our example in this notebook,  k = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer input to hidden parameters shape (W_x): torch.Size([12, 4])\n",
      "LSTM layer hidden to hidden parameters shape (W_x): torch.Size([12, 3])\n",
      "inputs: \n",
      " [tensor([[1.4934, 0.4987, 0.2319, 1.1746]], requires_grad=True), tensor([[-1.3967,  0.8998,  1.0956, -0.5231]], requires_grad=True), tensor([[-0.8462, -0.9946,  0.6311,  0.5327]], requires_grad=True), tensor([[-0.8454,  0.9406, -2.1224,  0.0233]], requires_grad=True), tensor([[ 0.4836,  1.2895,  0.8957, -0.2465]], requires_grad=True)]\n",
      "\n",
      "\n",
      "out: \n",
      " tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward0>), tensor([[[-0.7344,  0.6209,  0.4191]]], grad_fn=<StackBackward0>))\n",
      "out: \n",
      " tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward0>), tensor([[[-0.5041,  0.2672,  0.6370]]], grad_fn=<StackBackward0>))\n",
      "out: \n",
      " tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward0>), tensor([[[-0.5552,  0.7909,  0.3300]]], grad_fn=<StackBackward0>))\n",
      "out: \n",
      " tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward0>), tensor([[[-0.5645,  1.0073,  0.6101]]], grad_fn=<StackBackward0>))\n",
      "out: \n",
      " tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward0>), tensor([[[-0.5311,  0.1181,  0.5304]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# All of the Tensors, which we'd use here are enabled with AutoGrad because, that's what we need \n",
    "# and we don't need deprecated Variable wrapper type for that anymore. When a normal Tensor needs \n",
    "# to be converted to a differentiable Tensor, we can use requires_grad_ method of the Tensor class, \n",
    "# which accepts a boolean value True as default.\n",
    "\n",
    "# Defines an LSTM with an input dim of 4 and hidden dim of 3\n",
    "# This expects to see 4 values as input and generates 3 values as output\n",
    "batch_size: int = 1  # Because we have a single example input sequence\n",
    "input_size: int = 4  # Input size denotes the input vector at each time step of the sequence\n",
    "hidden_size: int = 3 # Hidden size is analogous to having that number of neurons in a hidden layer\n",
    "num_layers: int = 1  # We have a single LSTM layer in the vertical direction rather than stacking LSTMs\n",
    "\n",
    "lstm: nn.LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers) \n",
    "\n",
    "\n",
    "\n",
    "# Since the four weight matrices are stacked on vertical axis to come up with a compact dimension \n",
    "# we'd see the shape (3,4) | (3,4) | (3,4) | (3,4) -> (12, 4) for weights connecting the input \n",
    "# to hidden state. Input has size 4 and hidden size is 3.\n",
    "print(f'LSTM layer input to hidden parameters shape (W_x): {lstm.weight_ih_l0.shape}')\n",
    "# Similarly in this case the weights are connecting hidden state from previous time step to the hidden \n",
    "# state of current time step, we'd see shape (3,3) | (3,3) | (3,3) | (3,3) -> (12, 3) for the weights \n",
    "print(f'LSTM layer hidden to hidden parameters shape (W_x): {lstm.weight_hh_l0.shape}')\n",
    "\n",
    "# Makes an input sequence having length 5 i.e., it has 5 time steps, where each time step has an \n",
    "# input of size R^input_size. This is important to remember, that we have a single input sequence \n",
    "# or in other words, a single training example in a single batch, hence batch_size = 1.\n",
    "input_sequence: List[torch.Tensor] = [torch.randn(1, input_size, requires_grad=True) for _ in range(5)]\n",
    "print('inputs: \\n', input_sequence)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Next we'd initialize the initial hidden state h0 and c0. Here, we need to keep in mind, what \n",
    "# these vectors really are. The h0 or generally h_t is our short term memory (STM), which  \n",
    "# also leads to the output for a given time step. The other component c0 or generally c_t is the \n",
    "# the long term memory (LTM).\n",
    "\n",
    "# We'd see, that the LSTM object yields two tensors, which we refer as out and hidden. The \n",
    "# out part can be called short term memory or h_t for time t. The hidden part encapsulates \n",
    "# two things the final hidden state at time t and long term memory at time t. Now in this \n",
    "# example we have a single LSTM layer and we have not mentioned, what to do with the final \n",
    "# hidden state (think of it as dense hidden layer activation). Hence, we'd see for the \n",
    "# following iteration over our five time steps the out and the first part and the final \n",
    "# hidden state are one and same. We have various options here, which we'd explore later.\n",
    "# For now, we'd have to keep these semantics in mind.\n",
    "\n",
    "# Initialize the hidden state\n",
    "# (1 layer, 1 batch_size, 3 outputs)\n",
    "# first tensor is the hidden state, h0\n",
    "# second tensor initializes the cell memory, c0\n",
    "h0: torch.Tensor = torch.randn(1, 1, hidden_size, requires_grad=True)\n",
    "c0: torch.Tensor = torch.randn(1, 1, hidden_size, requires_grad=True)\n",
    "\n",
    "# step through the sequence one element at a time.\n",
    "for timestep_item in input_sequence:\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(timestep_item.view(1, 1, -1), (h0, c0))\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the output and hidden Tensors are always of length 3, which we specified when we defined the LSTM with `hidden_size`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All at once\n",
    "\n",
    "A for loop is not very efficient for large sequences of data, so we can also, **process all of these inputs at once.** This means instead of processing each time step one by one. We can prepare the full form of the input sequence by combining the parts for all time steps.\n",
    "\n",
    "1. Concatenate all our time step input i.e. $x_t$ into one big tensor, with a defined batch_size, which in this case would still be 1.\n",
    "2. Define the shape of our hidden state, which would also be the same. \n",
    "3. Get the outputs and the *most recent* hidden state (created after the last word in the sequence has been seen).\n",
    "\n",
    "\n",
    "The outputs may look slightly different due to our differently initialized hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs size: \n",
      " torch.Size([5, 1, 4])\n",
      "\n",
      "\n",
      "inputs: \n",
      " tensor([[[ 1.4934,  0.4987,  0.2319,  1.1746]],\n",
      "\n",
      "        [[-1.3967,  0.8998,  1.0956, -0.5231]],\n",
      "\n",
      "        [[-0.8462, -0.9946,  0.6311,  0.5327]],\n",
      "\n",
      "        [[-0.8454,  0.9406, -2.1224,  0.0233]],\n",
      "\n",
      "        [[ 0.4836,  1.2895,  0.8957, -0.2465]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "\n",
      "out: \n",
      " tensor([[[ 0.1611,  0.2200,  0.2213]],\n",
      "\n",
      "        [[ 0.0364, -0.0390,  0.2638]],\n",
      "\n",
      "        [[-0.1425, -0.0174,  0.1504]],\n",
      "\n",
      "        [[-0.1583,  0.1264,  0.1709]],\n",
      "\n",
      "        [[-0.2007, -0.1559,  0.2489]]], grad_fn=<StackBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.2007, -0.1559,  0.2489]]], grad_fn=<StackBackward0>), tensor([[[-0.4429, -0.2975,  0.3252]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Turn inputs into a tensor with 5 rows of data\n",
    "# add the extra 2nd dimension (1) for batch_size\n",
    "# This inputs array represents a complete sequence spread over 5 time steps together.\n",
    "inputs: torch.Tensor = torch.cat(input_sequence).view(len(input_sequence), batch_size, -1).requires_grad_()\n",
    "\n",
    "# print out our inputs and their shape\n",
    "# you should see (number of sequences, batch size, input_dim)\n",
    "print('inputs size: \\n', inputs.size())\n",
    "print('\\n')\n",
    "\n",
    "print('inputs: \\n', inputs)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "h0: torch.Tensor = torch.randn(1, 1, hidden_size, requires_grad=True)\n",
    "c0: torch.Tensor = torch.randn(1, 1, hidden_size, requires_grad=True)\n",
    "\n",
    "\n",
    "# get the outputs and hidden state\n",
    "out, hidden = lstm(inputs, (h0, c0))\n",
    "\n",
    "print('out: \\n', out)\n",
    "print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In context of the `out` and `hidden` states we now see, that the last row of the `out` tensor is exactly same as first part of the final hidden state. This is expected, because first part of the final hidden state is the short term memory from the last time step. And as we mentioned before since we have a single LSTM layer the `out` from a given time step is same as the short term memory from that time step. In this case last row of the out naturally belongs to the last time step.\n",
    "\n",
    "Now, we'd brings this knowledge to the next notebook, where we'd consider a concrete LSTM training scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: Hidden State and Gates\n",
    "\n",
    "This notebooks shows you the structure of the input and output of an LSTM in PyTorch. Next, you'll learn more about how exactly an LSTM represents long-term and short-term memory in it's hidden state, and you'll reach the next notebook exercise.\n",
    "\n",
    "#### Part of Speech\n",
    "\n",
    "In the notebook that comes later in this lesson, you'll see how to define a model to tag parts of speech (nouns, verbs, determinants), include an LSTM and a Linear layer to define a desired output size, *and* finally train our model to create a distribution of class scores that associates each input word with a part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
