{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow\n",
    "\n",
    "Optical flow tracks objects by looking at where the *same* points have moved from one image frame to the next. Let's load in a few example frames of a pacman-like face moving to the right and down and see how optical flow finds **motion vectors** that describe the motion of the face!\n",
    "\n",
    "As usual, let's first import our resources and read in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg  # for reading in images\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # computer vision library\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the image frames\n",
    "frame_1 = cv2.imread('images/pacman_1.png')\n",
    "frame_2 = cv2.imread('images/pacman_2.png')\n",
    "frame_3 = cv2.imread('images/pacman_3.png')\n",
    "\n",
    "# convert to RGB\n",
    "frame_1 = cv2.cvtColor(frame_1, cv2.COLOR_BGR2RGB)\n",
    "frame_2 = cv2.cvtColor(frame_2, cv2.COLOR_BGR2RGB)\n",
    "frame_3 = cv2.cvtColor(frame_3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Visualize the individual color channels\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))\n",
    "ax1.set_title('frame 1')\n",
    "ax1.imshow(frame_1)\n",
    "ax2.set_title('frame 2')\n",
    "ax2.imshow(frame_2)\n",
    "ax3.set_title('frame 3')\n",
    "ax3.imshow(frame_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Points to Track\n",
    "\n",
    "Befor optical flow can work, we have to give it a set of *keypoints* to track between two image frames!\n",
    "\n",
    "In the below example, we use a **Shi-Tomasi corner detector**, which uses the same process as a Harris corner detector to find patterns of intensity that make up a \"corner\" in an image, only it adds an additional parameter that helps select the most prominent corners. You can read more about this detection algorithm in [the documentation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_shi_tomasi/py_shi_tomasi.html). \n",
    "\n",
    "Alternatively, you could choose to use Harris or even ORB to find feature points. I just found that this works well.\n",
    "\n",
    "**You sould see that the detected points appear at the corners of the face.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 10,\n",
    "                       qualityLevel = 0.2,\n",
    "                       minDistance = 5,\n",
    "                       blockSize = 5 )\n",
    "\n",
    "\n",
    "# convert all frames to grayscale\n",
    "gray_1 = cv2.cvtColor(frame_1, cv2.COLOR_RGB2GRAY)\n",
    "gray_2 = cv2.cvtColor(frame_2, cv2.COLOR_RGB2GRAY)\n",
    "gray_3 = cv2.cvtColor(frame_3, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "# Take first frame and find corner points in it\n",
    "pts_1 = cv2.goodFeaturesToTrack(gray_1, mask = None, **feature_params)\n",
    "\n",
    "# display the detected points\n",
    "plt.imshow(frame_1)\n",
    "for p in pts_1:\n",
    "    # plot x and y detected points\n",
    "    plt.plot(p[0][0], p[0][1], 'r.', markersize=15)\n",
    "\n",
    "# print out the x-y locations of the detected points\n",
    "print(pts_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Optical Flow\n",
    "\n",
    "Once we've detected keypoints on our initial image of interest, we can calculate the optical flow between this image frame (frame 1) and the next frame (frame 2), using OpenCV's `calcOpticalFlowPyrLK` which is [documented, here](https://docs.opencv.org/trunk/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323). It takes in an initial image frame, the next image, and the first set of points, and it returns the detected points in the next frame and a value that indicates how good matches are between points from one frame to the next.\n",
    "\n",
    "The parameters also include a window size and maxLevels that indicate the size of a window and mnumber of levels that will be used to scale the given images using pyramid scaling; this version peforms an iterative search for matching points and this matching criteria is reflected in the last parameter (you may need to change these values if you are working with a different image, but these should work for the provided example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (5,5),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "\n",
    "# calculate optical flow between first and second frame\n",
    "pts_2, match, err = cv2.calcOpticalFlowPyrLK(gray_1, gray_2, pts_1, None, **lk_params)\n",
    "\n",
    "# Select good matching points between the two image frames\n",
    "good_new = pts_2[match==1]\n",
    "good_old = pts_1[match==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's display the resulting motion vectors! You should see the first image with motion vectors drawn on it that indicate the direction of motion from the first frame to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask image for drawing (u,v) vectors on top of the second frame\n",
    "mask = np.zeros_like(frame_2)\n",
    "\n",
    "# draw the lines between the matching points (these lines indicate motion vectors)\n",
    "for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "    a,b = new.ravel()\n",
    "    c,d = old.ravel()\n",
    "    # draw points on the mask image\n",
    "    mask = cv2.circle(mask,(a,b),5,(200),-1)\n",
    "    # draw motion vector as lines on the mask image\n",
    "    mask = cv2.line(mask, (a,b),(c,d), (200), 3)\n",
    "    # add the line image and second frame together\n",
    "\n",
    "composite_im = np.copy(frame_2)\n",
    "composite_im[mask!=0] = [0]\n",
    "\n",
    "plt.imshow(composite_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Perform Optical Flow between image frames 2 and 3\n",
    "\n",
    "Repeat this process but for the last two image frames; see what the resulting motion vectors look like. Imagine doing this for a series of image frames and plotting the entire-motion-path of a given object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Perform optical flow between image frames 2 and 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
